<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Eric Pinzur">
<meta name="dcterms.date" content="2023-09-13">
<meta name="keywords" content="few-shot learning, fine-tuning, gen-ai, kaskada, llm, python, open-ai">

<title>Intro to Fine-Tuning LLMs</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/cookie-consent/cookie-consent.js"></script>
<link href="../../site_libs/cookie-consent/cookie-consent.css" rel="stylesheet">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-0TFWEQ26VG"></script>

<script type="text/plain" cookie-consent="tracking">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-0TFWEQ26VG', { 'anonymize_ip': true});
</script>

<script type="text/javascript" charset="UTF-8">
document.addEventListener('DOMContentLoaded', function () {
cookieconsent.run({
  "notice_banner_type":"simple",
  "consent_type":"implied",
  "palette":"light",
  "language":"en",
  "page_load_consent_levels":["strictly-necessary","functionality","tracking","targeting"],
  "notice_banner_reject_button_hide":false,
  "preferences_center_close_button_hide":false,
  "website_name":""
  });
});
</script> 
  


<link rel="stylesheet" href="../../styles.css">
<link rel="stylesheet" href="styles.css">
<meta property="og:title" content="Intro to Fine-Tuning LLMs">
<meta property="og:description" content="Data Science for non Data Scientists">
<meta property="og:image" content="https://github.com/epinzur/epinzur.github.io/posts/001-openai-fine-tuning-1/intro_to_llms.jpg">
<meta property="og:site-name" content="epinzur.github.io">
<meta property="og:image:alt" content="3d circuitry">
<meta name="twitter:title" content="Intro to Fine-Tuning LLMs">
<meta name="twitter:description" content="Data Science for non Data Scientists">
<meta name="twitter:image" content="https://github.com/epinzur/epinzur.github.io/posts/001-openai-fine-tuning-1/intro_to_llms.jpg">
<meta name="twitter:image:alt" content="3d circuitry">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">epinzur.github.io</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/epinzur" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/epinzur" rel="" target=""><i class="bi bi-linkedin" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <div class="quarto-title-block"><div><h1 class="title">Intro to Fine-Tuning Large Language Models (LLMs)</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li></ul></div></div>
            <p class="subtitle lead">Data Science for non Data Scientists</p>
                                <div class="quarto-categories">
                <div class="quarto-category">few-shot learning</div>
                <div class="quarto-category">fine-tuning</div>
                <div class="quarto-category">gen-ai</div>
                <div class="quarto-category">kaskada</div>
                <div class="quarto-category">llm</div>
                <div class="quarto-category">python</div>
                <div class="quarto-category">open-ai</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>Eric Pinzur </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">September 13, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active" data-toc-expanded="99">
    <h2 id="toc-title">In this post</h2>
   
  <ul>
  <li><a href="#intro" id="toc-intro" class="nav-link active" data-scroll-target="#intro">Intro</a></li>
  <li><a href="#definitions" id="toc-definitions" class="nav-link" data-scroll-target="#definitions">Definitions</a></li>
  <li><a href="#building-training-examples" id="toc-building-training-examples" class="nav-link" data-scroll-target="#building-training-examples">Building Training Examples</a>
  <ul class="collapse">
  <li><a href="#hypothesis-generation" id="toc-hypothesis-generation" class="nav-link" data-scroll-target="#hypothesis-generation">Hypothesis generation</a></li>
  <li><a href="#example-construction" id="toc-example-construction" class="nav-link" data-scroll-target="#example-construction">Example construction</a></li>
  <li><a href="#formatting-examples" id="toc-formatting-examples" class="nav-link" data-scroll-target="#formatting-examples">Formatting examples</a></li>
  <li><a href="#training-example-cleanup" id="toc-training-example-cleanup" class="nav-link" data-scroll-target="#training-example-cleanup">Training example cleanup</a></li>
  </ul></li>
  <li><a href="#refining-training-examples" id="toc-refining-training-examples" class="nav-link" data-scroll-target="#refining-training-examples">Refining Training Examples</a>
  <ul class="collapse">
  <li><a href="#determining-signal-strength" id="toc-determining-signal-strength" class="nav-link" data-scroll-target="#determining-signal-strength">Determining signal strength</a></li>
  <li><a href="#example-validation" id="toc-example-validation" class="nav-link" data-scroll-target="#example-validation">Example validation</a></li>
  </ul></li>
  <li><a href="#model-fine-tuning-1" id="toc-model-fine-tuning-1" class="nav-link" data-scroll-target="#model-fine-tuning-1">Model Fine-Tuning</a>
  <ul class="collapse">
  <li><a href="#upload-training-data" id="toc-upload-training-data" class="nav-link" data-scroll-target="#upload-training-data">Upload training data</a></li>
  <li><a href="#create-a-fine-tuning-job" id="toc-create-a-fine-tuning-job" class="nav-link" data-scroll-target="#create-a-fine-tuning-job">Create a fine-tuning job</a></li>
  <li><a href="#wait-for-the-job-to-finish" id="toc-wait-for-the-job-to-finish" class="nav-link" data-scroll-target="#wait-for-the-job-to-finish">Wait for the job to finish</a></li>
  <li><a href="#using-the-fine-tuned-model" id="toc-using-the-fine-tuned-model" class="nav-link" data-scroll-target="#using-the-fine-tuned-model">Using the fine-tuned model</a></li>
  </ul></li>
  <li><a href="#model-validation" id="toc-model-validation" class="nav-link" data-scroll-target="#model-validation">Model Validation</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="intro" class="level1">
<h1>Intro</h1>
<p>As a contributor to the <a href="http://kaskada.io/kaskada">Kaskada</a> open-source project, I’ve always been driven by the immense potential of today’s technology. In this post, I’ll discuss my learnings around fine-tuning large language models (LLMs). As an example, I’ll use my work on <a href="https://github.com/kaskada-ai/beep-gpt">BeepGPT</a>, where I set out to build a customized, real-time model to predict which Slack conversations might different users find interesting.</p>
<p>First, I’d like to say that I am not a Data Scientist. I’ve worked with a few Data Scientists, but my knowledge in the space is limited. With the advent of high-quality LLMs, software engineers like me can now delve into Data Science with little-to-no Data Science background.</p>
<p>So, why should you care? Because the line between Data Science professionals and enthusiasts like you and me is becoming delightfully blurred. Dive in to explore how I ventured into the domain of Data Scientists and what I uncovered along the way.</p>
<section id="here-are-a-few-tips-before-we-get-started" class="level4">
<h4 class="anchored" data-anchor-id="here-are-a-few-tips-before-we-get-started">Here are a few tips before we get started</h4>
<p>First is a willingness to experiment. Model fine-tuning is an iterative process. The first way you build your training examples will likely not produce a successful model. When working on BeepGPT I experimented with five different approaches (over a week) before I found one that was successful at predicting user interest.</p>
<p>Second is the importance of data quality. When fine-tuning a model, numerous training examples are sent to a model to update its behavior. The examples must contain strong signals that relate to the desired output. Despite the recent advances made with LLMs, garbage in still leads to garbage out. <span class="citation" data-cites="gigo">(see: <a href="#ref-gigo" role="doc-biblioref">Ron Ozminkowski, PhD 2021</a>)</span></p>
<p>One trick I developed was using LLMs to help make decisions about the quality of each example. With few-shot learning, I taught a model what strong signals look like, and I was able to use that to enhance the quality of my dataset.</p>
<p>Finally, it is vital to have many training examples. Ideally, a fine-tuning job should be run with at least a few thousand training examples. The more examples you can provide to the model, the more it can learn, and the better the predictions will be in production.</p>
</section>
</section>
<section id="definitions" class="level1">
<h1>Definitions</h1>
<section id="large-language-model-llm" class="level4">
<h4 class="anchored" data-anchor-id="large-language-model-llm">Large Language Model (LLM)</h4>
<p>A large language model is an artificial intelligence (AI) algorithm that uses deep learning techniques and massively large data sets to understand, summarize, generate, and predict new content. <span class="citation" data-cites="llm">(<a href="#ref-llm" role="doc-biblioref">Sean Michael Kerner 2023</a>)</span></p>
</section>
<section id="tokens" class="level4">
<h4 class="anchored" data-anchor-id="tokens">Tokens</h4>
<p>LLMs process text as tokens instead of as characters. Tokens represent sets of characters that commonly occur in a specific sequence. On average, a token represents about four characters. You can use <a href="https://platform.openai.com/tokenizer">OpenAI’s tokenizer tool</a> to see how different texts get converted into tokens, for example:</p>
<div class="sourceCode">
<pre class="sourceCode code-with-copy"><span class="tokenizer-tkn tokenizer-tkn-0" title="15592">Team</span><span class="tokenizer-tkn tokenizer-tkn-1" title="11">,</span><span class="tokenizer-tkn tokenizer-tkn-2" title="750"> did</span><span class="tokenizer-tkn tokenizer-tkn-3" title="345"> you</span><span class="tokenizer-tkn tokenizer-tkn-4" title="2883"> enjoy</span><span class="tokenizer-tkn tokenizer-tkn-0" title="262"> the</span><span class="tokenizer-tkn tokenizer-tkn-1" title="299"> n</span><span class="tokenizer-tkn tokenizer-tkn-2" title="620">ach</span><span class="tokenizer-tkn tokenizer-tkn-3" title="418">os</span><span class="tokenizer-tkn tokenizer-tkn-4" title="7415"> yesterday</span><span class="tokenizer-tkn 
tokenizer-tkn-0" title="30">?</span><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre>
</div>
<p>The color highlighting shows how 41 characters become 11 tokens. You can mouse over to see the actual token values.</p>
<p>Common words and most positive integers under 1000 equate to a single token. Whitespace and capitalization matter: <code> Team</code>, <code> team</code>, <code>Team</code>, and <code>team</code> equate to four different tokens.</p>
</section>
<section id="prompts-completions" class="level4">
<h4 class="anchored" data-anchor-id="prompts-completions">Prompts &amp; Completions</h4>
<p>Prompts are the input to LLMs. If you have played with ChatGPT before, the request you sent was a prompt.</p>
<p>Completions are the outputs from LLMs. With ChatGPT, the response to your question was a completion.</p>
</section>
<section id="training-examples" class="level4">
<h4 class="anchored" data-anchor-id="training-examples">Training Examples</h4>
<p>Training examples are prompt &amp; completion pairs. The prompt is the text we would have sent to the model, and the completion is the response we would have expected.</p>
</section>
<section id="maximum-token-length" class="level4">
<h4 class="anchored" data-anchor-id="maximum-token-length">Maximum Token Length</h4>
<p>The maximum length of an API request to a model, in tokens. Depending on the model, there is a different maximum length.</p>
</section>
<section id="few-shot-learning" class="level4">
<h4 class="anchored" data-anchor-id="few-shot-learning">Few-Shot Learning</h4>
<p>Few-shot learning is a method of working with language models to improve response quality for specific tasks. It works by including training examples on each request to a model, demonstrating the desired response behavior. For example, when playing with ChatGPT:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode default code-overflow-wrap code-with-copy"><code class="sourceCode default"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a>&gt; I'd like you to identify if an animal passed to you is a bird or not-a-bird. Here are some examples:</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>&gt; Penguin: bird</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>&gt; Rabbit: not-a-bird</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>&gt; Parrot:</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="center">
<p><code>Penguin: bird</code> and <code>Rabbit: not-a-bird</code> are few-shot examples.</p>
</div>
<p>Few-shot learning helps adapt an existing LLM to perform a new task quickly. However, it is limited by the Maximum Token Length of the model. If you are trying to train a model to do a complex behavior, you might not be able to include all your desired training examples in a single request. Additionally, the cost and response time increase as the number of tokens you send increases.</p>
</section>
<section id="model-fine-tuning" class="level4">
<h4 class="anchored" data-anchor-id="model-fine-tuning">Model Fine-Tuning</h4>
<p>Fine-tuning is the process of re-training existing base language models (like GPT3) with new datasets to perform specific tasks. Fine-tuning improves on few-shot learning by training on many more examples than can fit in a single request.</p>
<p>Continuing the previous example, we could fine-tune an existing model by providing a file containing hundreds of animals labeled <code>bird</code> or <code>not-a-bird</code>. The output of this would be a new model that would be more efficient at this specific task.</p>
<p>Making requests to a fine-tuned model is generally cheaper and faster than sending few-shot learning requests. However, the process of fine-tuning a model itself can be quite expensive.</p>
<p>It is important to weigh the tradeoffs between few-shot learning and model fine-tuning.</p>
</section>
</section>
<section id="building-training-examples" class="level1">
<h1>Building Training Examples</h1>
<section id="hypothesis-generation" class="level3">
<h3 class="anchored" data-anchor-id="hypothesis-generation">Hypothesis generation</h3>
<p>Before we start building training examples, you need to form hypotheses about what you want to predict and how you might do so successfully. This is where the iterative process starts.</p>
<p>For BeepGPT I experimented with the following ideas:</p>
<ul>
<li>For a set of recent messages in a channel, try to predict:
<ul>
<li>The reaction (if any) to the most recent message</li>
<li>The next user that will reply</li>
<li>The set of users that might interact next (reply or react)</li>
</ul></li>
<li>For the set of recent messages in a conversation, try to predict:
<ul>
<li>The set of users that might interact next</li>
<li>The next user that will reply</li>
</ul></li>
</ul>
<p>Note that I didn’t initially develop all of these ideas, only the first one. I built out training examples for each idea and tried to fine-tune a model. When the results weren’t as good as expected, I moved on to the next idea. Experimentation is key.</p>
<p>Ultimately, I was most successful with the final idea: Predict the next user to reply to the set of recent messages in a conversation. The rest of the post will focus on this.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tip
</div>
</div>
<div class="callout-body-container callout-body">
<p>I used <a href="http://kaskada.io/kaskada">Kaskada</a> to iterate on these ideas quickly. Kaskada is a tool that makes it easy to collect and aggregate events from raw data. You don’t need to preprocess anything. Just import the raw events and start experimenting. Furthermore, Kaskada ensures that your examples will not be subject to leakage, which is a big problem in predictive modeling. In a future post, I’ll show how I used Kaskada to generate training examples for each of the above ideas.</p>
</div>
</div>
</section>
<section id="example-construction" class="level3">
<h3 class="anchored" data-anchor-id="example-construction">Example construction</h3>
<p>Consider this conversation:</p>
<div class="hanging-indent margin-0">
<p><strong>UserA</strong>: <code>Team, did you enjoy the nachos yesterday?</code></p>
<p><strong>UserB</strong>: <code>Yes, I love Mexican food.</code></p>
<p><strong>UserA</strong>: <code>&lt;@UserC&gt; I'm trying to get my application deployed in Kubernetes. However, I can't determine whether to use a deployment or a stateful set. I found some docs here: http://tinyurl.com/4k53dc8h, but I'm still unsure which to choose. Can you help?</code></p>
<p><strong>UserB</strong>: <code>UserC is at lunch now. They will be back in about an hour. I don't know much about this either, but I can try to help. Or is it okay to wait until UserC returns?</code></p>
<p><strong>UserA</strong>: <code>I can wait for UserC to return.</code></p>
<p><strong>UserC</strong>: <code>I can help with this. Can you tell me more about your application? Does it have any persistent storage requirements?</code></p>
</div>
<p>Reviewing the definition of <em>Training Examples</em>, it states: “Training examples are prompt &amp; completion pairs. The prompt is the text we would have sent to the model, and the completion is the response we would have expected.”</p>
<p>In BeepGPT, remember that we are trying to predict who might respond next in a conversation. We want the model to understand how users interact based on their interests, social relationships, responsibilities, etc. Therefore, we build the <em>prompt</em> from the previous messages in the conversation. For the <em>completion</em>, we will use the event we extracted from our data (the next user to reply) as the training signal.</p>
<p>From the first two messages in the conversation, we can generate a training example. The prompt is the first message, and the completion is the user that responded:</p>
<div class="hanging-indent margin-0">
<p><strong>prompt</strong>: <code>Team, did you enjoy the nachos yesterday?</code> <strong>completion</strong>: <code>UserB</code></p>
</div>
<p>Instead, if we consider the last four messages we can build another training example. Note, we use two new-line characters (<code>\n\n</code>) to join messages:</p>
<div class="hanging-indent margin-0">
<p><strong>prompt</strong>: <code>&lt;@UserC&gt; I'm trying to get my application deployed in Kubernetes. However, I can't determine whether to use a deployment or a stateful set. I found some docs here: http://tinyurl.com/4k53dc8h, but I'm still unsure which to choose. Can you help?\n\nUserC is at lunch now. They will be back in about an hour. I don't know much about this either, but I can try to help. Or is it okay to wait until UserC returns?\n\nI can wait for UserC to return.</code></p>
<p><strong>completion</strong>: <code>UserC</code></p>
</div>
<p>When using the OpenAI fine-tuning API, each training example should be a blob of JSON in a specific format on a single line. Converting our two examples above, we now have the following:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode json code-overflow-wrap code-with-copy"><code class="sourceCode json"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">"prompt"</span><span class="fu">:</span> <span class="st">"Team, did you enjoy the nachos yesterday?"</span><span class="fu">,</span> <span class="dt">"completion"</span><span class="fu">:</span> <span class="st">"UserB"</span><span class="fu">}</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">"prompt"</span><span class="fu">:</span> <span class="st">"&lt;@UserC&gt; I'm trying to get my application deployed in Kubernetes. However, I can't determine whether to use a deployment or a stateful set. I found some docs here: http://tinyurl.com/4k53dc8h, but I'm still unsure which to choose. Can you help?</span><span class="ch">\n\n</span><span class="st">UserC is at lunch now. They will be back in about an hour. I don't know much about this either, but I can try to help. Or is it okay to wait until UserC returns?</span><span class="ch">\n\n</span><span class="st">I can wait for UserC to return."</span><span class="fu">,</span> <span class="dt">"completion"</span><span class="fu">:</span> <span class="st">"UserC"</span><span class="fu">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="formatting-examples" class="level3">
<h3 class="anchored" data-anchor-id="formatting-examples">Formatting examples</h3>
<p>Next, there are several formatting rules that you are recommended to follow. I don’t understand why these are recommended, but I followed them anyway.</p>
<ul>
<li>All prompts should end with the same set of characters. The set of characters used should not occur elsewhere in your dataset. The recommended string for textual input data is <code>\n\n###\n\n</code>.</li>
<li>All completions should start with a single whitespace character.</li>
</ul>
<p>Applying these rules to our examples, we get:</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode json code-overflow-wrap code-with-copy"><code class="sourceCode json"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">"prompt"</span><span class="fu">:</span> <span class="st">"Team, did you enjoy the nachos yesterday?</span><span class="ch">\n\n</span><span class="st">###</span><span class="ch">\n\n</span><span class="st">"</span><span class="fu">,</span> <span class="dt">"completion"</span><span class="fu">:</span> <span class="st">" UserB"</span><span class="fu">}</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">"prompt"</span><span class="fu">:</span> <span class="st">"&lt;@UserC&gt; I'm trying to get my application deployed in Kubernetes. However, I can't determine whether to use a deployment or a stateful set. I found some docs here: http://tinyurl.com/4k53dc8h, but I'm still unsure which to choose. Can you help?</span><span class="ch">\n\n</span><span class="st">UserC is at lunch now. They will be back in about an hour. I don't know much about this either, but I can try to help. Or is it okay to wait until UserC returns?</span><span class="ch">\n\n</span><span class="st">I can wait for UserC to return.</span><span class="ch">\n\n</span><span class="st">###</span><span class="ch">\n\n</span><span class="st">"</span><span class="fu">,</span> <span class="dt">"completion"</span><span class="fu">:</span> <span class="st">" UserC"</span><span class="fu">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="training-example-cleanup" class="level3">
<h3 class="anchored" data-anchor-id="training-example-cleanup">Training example cleanup</h3>
<p>Finally, I found that model training works best if the following is done:</p>
<ul>
<li>Non-textual data like http-links, code blocks, and IDs are removed from the prompts.</li>
<li>Completions are reduced to a single token in length.</li>
</ul>
<p>We can use regex and other string functions to remove non-textual data from the prompts. And we can use standard data science tools like the <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html">Scikit-Learn LabelEncoder</a> to create a mapping from UserIds to integers. Remember that positive integers under one thousand map to unique tokens.</p>
<p>So now we have:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode json code-overflow-wrap code-with-copy"><code class="sourceCode json"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">"prompt"</span><span class="fu">:</span> <span class="st">"Team, did you enjoy the nachos yesterday?</span><span class="ch">\n\n</span><span class="st">###</span><span class="ch">\n\n</span><span class="st">"</span><span class="fu">,</span> <span class="dt">"completion"</span><span class="fu">:</span> <span class="st">" 1"</span><span class="fu">}</span></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">"prompt"</span><span class="fu">:</span> <span class="st">"I'm trying to get my application deployed in Kubernetes. However, I can't determine whether to use a deployment or a stateful set. I found some docs here:, but I'm still unsure which to choose. Can you help?</span><span class="ch">\n\n</span><span class="st">UserC is at lunch now. They will be back in about an hour. I don't know much about this either, but I can try to help. Or is it okay to wait until UserC returns?</span><span class="ch">\n\n</span><span class="st">I can wait for UserC to return.</span><span class="ch">\n\n</span><span class="st">###</span><span class="ch">\n\n</span><span class="st">"</span><span class="fu">,</span> <span class="dt">"completion"</span><span class="fu">:</span> <span class="st">" 2"</span><span class="fu">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>Removing IDs was especially helpful. Before I did so, the model essentially learned how to map from input UserId to output user. It skipped learning anything valuable from the actual text of the messages.</p>
</div>
</div>
<p>We now have two training examples that we could use for fine-tuning a model.</p>
<p>Ideally, for fine-tuning, we would have several thousand examples. Using a tool like <a href="http://kaskada.io/kaskada">Kaskada</a>, generating examples like this from your entire Slack history should be relatively easy.</p>
</section>
</section>
<section id="refining-training-examples" class="level1">
<h1>Refining Training Examples</h1>
<p>Before proceeding with fine-tuning, I recommend taking the following steps:</p>
<ol type="1">
<li>Use few-shot learning to determine which examples contain strong signals to be helpful.</li>
<li>Use the OpenAI CLI to validate that the training examples are in the correct format.</li>
</ol>
<section id="determining-signal-strength" class="level3">
<h3 class="anchored" data-anchor-id="determining-signal-strength">Determining signal strength</h3>
<section id="overview" class="level5">
<h5 class="anchored" data-anchor-id="overview">Overview</h5>
<p>I found that it is best if each training example contains strong signals to predict your desired outcome. If an example doesn’t have enough signal, you should consider excluding it from your training set.</p>
<p>Depending on your goal, including some negative examples may also be helpful. Negative examples help train the model about when not to make a prediction or stated another way, about when to predict that no action should be taken.</p>
<p>For example, with BeepGPT we are trying to predict when a set of messages might be interesting for a specific user. If we look at our training examples from the previous section, the first does not contain anything interesting. We would not want to alert anyone about this message. Therefore, we should convert this example into a negative example.</p>
<p>The second example does contain a strong signal. Here, we would like to alert users who have previously answered questions about Kubernetes. This example should be left as is.</p>
<p>To convert an example to a negative one, we must change its completion to indicate a non-response. I chose to use <code> nil</code> for this, which is represented by a single token in OpenAI.</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode json code-overflow-wrap code-with-copy"><code class="sourceCode json"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">"prompt"</span><span class="fu">:</span> <span class="st">"Team, did you enjoy the nachos yesterday?</span><span class="ch">\n\n</span><span class="st">###</span><span class="ch">\n\n</span><span class="st">"</span><span class="fu">,</span> <span class="dt">"completion"</span><span class="fu">:</span> <span class="st">" nil"</span><span class="fu">}</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="fu">{</span><span class="dt">"prompt"</span><span class="fu">:</span> <span class="st">"I'm trying to get my application deployed in Kubernetes. However, I can't determine whether to use a deployment or a stateful set. I found some docs here: but I'm still unsure which to choose. Can you help?</span><span class="ch">\n\n</span><span class="st">UserC is at lunch now. They will be back in about an hour. I don't know much about this either, but I can try to help. Or is it okay to wait until UserC returns?</span><span class="ch">\n\n</span><span class="st">I can wait for UserC to return.</span><span class="ch">\n\n</span><span class="st">###</span><span class="ch">\n\n</span><span class="st">"</span><span class="fu">,</span> <span class="dt">"completion"</span><span class="fu">:</span> <span class="st">" 2"</span><span class="fu">}</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="automating-this-process" class="level5">
<h5 class="anchored" data-anchor-id="automating-this-process">Automating this process</h5>
<p>Instead of manually going through each generated example to determine if it should be positive or negative, we can use a model with few-shot learning to do this work.</p>
<div class="callout callout-style-default callout-tip callout-titled" title="Game Changer">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Game Changer
</div>
</div>
<div class="callout-body-container callout-body">
<p>I developed this trick of using few-shot learning to improve training data quality. It didn’t seem like it would work initially, but it was surprisingly effective. I think this is potentially a game-changing technique for enhancing the fine-tuning of LLMs.</p>
</div>
</div>
<p>To start with few-shot learning, we need to find a few examples that we will provide to the model to make decisions on our behalf. Look through your generated examples and try to find 20-30 for each bucket: <strong>positive</strong> and <strong>negative</strong>. Add these to files: <code>examples_pos.jsonl</code> and <code>examples_neg.jsonl</code></p>
<p>Positive (strong signal) examples:</p>
<ul>
<li>I’ve been utilizing the Rust syntax highlighter for my code blocks. It does a good job of differentiating between functions and literals.</li>
<li>The agent doesn’t push to Prometheus; this is just another proxy location that Prometheus scrapes.</li>
</ul>
<p>Negative (weak signal) examples:</p>
<ul>
<li>There are some very interesting ideas here. thx for sharing.</li>
<li>Were there any issues with this? I’ll start verifying a few things in a bit.</li>
<li>Standup?</li>
</ul>
<p>We will now use OpenAI’s ChatCompletion API with few-shot learning to iterate over our complete set of training examples and label each as positive or negative.</p>
<p>First, we will generate an instruction set for the model by building up an array of messages in JSON. Each message object contains <code>role</code> and <code>content</code> properties. The <code>role</code> can be either <code>system</code>, <code>user</code>, or <code>assistant</code>.</p>
<p>The first message should always be from the <code>system</code> role and provide general instructions to the model of its function. Following this, message pairs of <code>user</code> and <code>assistant</code> should be added, where the <code>user</code> content is our example input and the <code>assistant</code> content is our expected response from the API. The model uses these few-shot training examples to help it determine our desired output.</p>
<p>Then, we append a final <code>user</code> message to the instruction set containing the content we want evaluated.</p>
<p>Open the code folds below to view some example Python code for performing this refinement.</p>
<div class="callout callout-style-default callout-warning callout-titled" title="Tips &amp; Warnings">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Tips &amp; Warnings
</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>This will cost a fair amount on OpenAI. A rough estimate is $50 per 10,000 examples.</li>
<li>This can take a long time to run to completion. The ChatCompletion API limits the number of tokens used per minute. In my experience, running 10,000 examples through this process takes about 8 hours.</li>
<li>The code is written in blocks to run inside a Jupyter Notebook environment.</li>
<li>If you want to run the code yourself, you will need an OpenAI API key.</li>
</ul>
</div>
</div>
<div class="cell">
<details>
<summary>Code: install required packages</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="op">%</span>pip install openai backoff numpy pandas scikit<span class="op">-</span>learn</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a><span class="co"># We use the `backoff` library to retry requests that have failed due to a </span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a><span class="co"># rate-limit error. Despite this addition, sometimes the process stalls and</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="co"># must be manually restarted. The code below appends to the output file instead</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a><span class="co"># of replacing it, so that the process can be resumed after an error occurs.</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="co"># numpy pandas scikit-learn are standard data science libraries we</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="co"># will use later in the process for a variety of tasks</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="15">
<details>
<summary>Code: import packages and init openAI with your API key</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> backoff, getpass, json, numpy, openai, pandas, sklearn, time</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>openai.api_key <span class="op">=</span> getpass.getpass(<span class="st">'OpenAI API Key:'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="5">
<details>
<summary>Code: init a bunch of stuff</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># This code assumes that you have a file named </span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a><span class="co"># `examples.jsonl`, which contains the full set </span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a><span class="co"># of training examples generated above.</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a><span class="co"># get a total count of examples in the input file</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>total_count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="ss">f'examples.jsonl'</span>, <span class="st">'r'</span>) <span class="im">as</span> in_file:</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> line <span class="kw">in</span> in_file:</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>        total_count <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a><span class="co"># initialize a progress counter</span></span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>success_count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a><span class="co"># build up the instruction set for few-shot learning</span></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a><span class="co"># start with a `system` message that provides the general instructions to the model</span></span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>system_instructions <span class="op">=</span> <span class="st">"You are a helpful assistant. Your job is to determine </span><span class="ch">\</span></span>
<span id="cb8-18"><a href="#cb8-18" aria-hidden="true" tabindex="-1"></a><span class="st">    if a prompt will help fine-tune a model. All prompts start with </span><span class="ch">\</span></span>
<span id="cb8-19"><a href="#cb8-19" aria-hidden="true" tabindex="-1"></a><span class="st">    'start --&gt;' and end with: '</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n###</span><span class="ch">\\</span><span class="st">n</span><span class="ch">\\</span><span class="st">n'. You should respond 'yes' if you </span><span class="ch">\</span></span>
<span id="cb8-20"><a href="#cb8-20" aria-hidden="true" tabindex="-1"></a><span class="st">    think the prompt has enough context to be helpful, or 'no' if not. No </span><span class="ch">\</span></span>
<span id="cb8-21"><a href="#cb8-21" aria-hidden="true" tabindex="-1"></a><span class="st">    explanation is needed. You should only respond with 'yes' or 'no'."</span></span>
<span id="cb8-22"><a href="#cb8-22" aria-hidden="true" tabindex="-1"></a>instructions <span class="op">=</span> [{<span class="st">"role"</span>: <span class="st">"system"</span>, <span class="st">"content"</span>: system_instructions}]</span>
<span id="cb8-23"><a href="#cb8-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-24"><a href="#cb8-24" aria-hidden="true" tabindex="-1"></a><span class="co"># then add the positive and negative examples that we </span></span>
<span id="cb8-25"><a href="#cb8-25" aria-hidden="true" tabindex="-1"></a><span class="co"># manually pulled out of the full set</span></span>
<span id="cb8-26"><a href="#cb8-26" aria-hidden="true" tabindex="-1"></a>pos <span class="op">=</span> <span class="bu">open</span>(<span class="ss">f'examples_pos.jsonl'</span>, <span class="st">'r'</span>)</span>
<span id="cb8-27"><a href="#cb8-27" aria-hidden="true" tabindex="-1"></a>neg <span class="op">=</span> <span class="bu">open</span>(<span class="ss">f'examples_neg.jsonl'</span>, <span class="st">'r'</span>)</span>
<span id="cb8-28"><a href="#cb8-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-29"><a href="#cb8-29" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb8-30"><a href="#cb8-30" aria-hidden="true" tabindex="-1"></a>    pos_line <span class="op">=</span> pos.readline()</span>
<span id="cb8-31"><a href="#cb8-31" aria-hidden="true" tabindex="-1"></a>    neg_line <span class="op">=</span> neg.readline()</span>
<span id="cb8-32"><a href="#cb8-32" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-33"><a href="#cb8-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> (<span class="kw">not</span> pos_line) <span class="kw">or</span> (<span class="kw">not</span> neg_line):</span>
<span id="cb8-34"><a href="#cb8-34" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span>
<span id="cb8-35"><a href="#cb8-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-36"><a href="#cb8-36" aria-hidden="true" tabindex="-1"></a>    pos_data <span class="op">=</span> json.loads(pos_line)</span>
<span id="cb8-37"><a href="#cb8-37" aria-hidden="true" tabindex="-1"></a>    neg_data <span class="op">=</span> json.loads(neg_line)</span>
<span id="cb8-38"><a href="#cb8-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-39"><a href="#cb8-39" aria-hidden="true" tabindex="-1"></a>    <span class="co"># alternate adding positive and negative examples</span></span>
<span id="cb8-40"><a href="#cb8-40" aria-hidden="true" tabindex="-1"></a>    instructions.append({<span class="st">"role"</span>: <span class="st">"user"</span>, <span class="st">"content"</span>: <span class="ss">f'start --&gt;</span><span class="sc">{</span>pos_data[<span class="st">"prompt"</span>]<span class="sc">}</span><span class="ss">'</span>})</span>
<span id="cb8-41"><a href="#cb8-41" aria-hidden="true" tabindex="-1"></a>    instructions.append({<span class="st">"role"</span>: <span class="st">"assistant"</span>, <span class="st">"content"</span>: <span class="st">"yes"</span>})</span>
<span id="cb8-42"><a href="#cb8-42" aria-hidden="true" tabindex="-1"></a>    instructions.append({<span class="st">"role"</span>: <span class="st">"user"</span>, <span class="st">"content"</span>: <span class="ss">f'start --&gt;</span><span class="sc">{</span>neg_data[<span class="st">"prompt"</span>]<span class="sc">}</span><span class="ss">'</span>})</span>
<span id="cb8-43"><a href="#cb8-43" aria-hidden="true" tabindex="-1"></a>    instructions.append({<span class="st">"role"</span>: <span class="st">"assistant"</span>,<span class="st">"content"</span>: <span class="st">"no"</span>})</span>
<span id="cb8-44"><a href="#cb8-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-45"><a href="#cb8-45" aria-hidden="true" tabindex="-1"></a>pos.close()</span>
<span id="cb8-46"><a href="#cb8-46" aria-hidden="true" tabindex="-1"></a>neg.close()</span>
<span id="cb8-47"><a href="#cb8-47" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-48"><a href="#cb8-48" aria-hidden="true" tabindex="-1"></a><span class="co"># setup a method to retry requests automatically</span></span>
<span id="cb8-49"><a href="#cb8-49" aria-hidden="true" tabindex="-1"></a><span class="at">@backoff.on_exception</span>(backoff.expo, (openai.error.RateLimitError, openai.error.ServiceUnavailableError))</span>
<span id="cb8-50"><a href="#cb8-50" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> chat_with_backoff(<span class="op">**</span>kwargs):</span>
<span id="cb8-51"><a href="#cb8-51" aria-hidden="true" tabindex="-1"></a>    <span class="co"># add an additional delay, because the first retry almost always fails</span></span>
<span id="cb8-52"><a href="#cb8-52" aria-hidden="true" tabindex="-1"></a>    time.sleep(<span class="dv">1</span>)</span>
<span id="cb8-53"><a href="#cb8-53" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb8-54"><a href="#cb8-54" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> openai.ChatCompletion.create(<span class="op">**</span>kwargs)</span>
<span id="cb8-55"><a href="#cb8-55" aria-hidden="true" tabindex="-1"></a>    <span class="cf">except</span> openai.error.InvalidRequestError:</span>
<span id="cb8-56"><a href="#cb8-56" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="va">None</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<details>
<summary>Code: iterate through each example to determine if it contains strong signals for fine-tuning</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># if this code block stalls, you can restart it to resume processing. </span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="co"># If you get an error about too many tokens used, reduce the number of </span></span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a><span class="co"># positive and negative examples in your generated instructions.</span></span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Or try to summarize the positive &amp; negative examples (manually or </span></span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a><span class="co"># with ChatGPT) to reduce their length.</span></span>
<span id="cb9-7"><a href="#cb9-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-8"><a href="#cb9-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> clear_output</span>
<span id="cb9-9"><a href="#cb9-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-10"><a href="#cb9-10" aria-hidden="true" tabindex="-1"></a>count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb9-11"><a href="#cb9-11" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="ss">f'examples.jsonl'</span>, <span class="st">'r'</span>) <span class="im">as</span> in_file:</span>
<span id="cb9-12"><a href="#cb9-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(<span class="ss">f'examples_refined.jsonl'</span>, <span class="st">'a'</span>) <span class="im">as</span> out_file:</span>
<span id="cb9-13"><a href="#cb9-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> line <span class="kw">in</span> in_file:</span>
<span id="cb9-14"><a href="#cb9-14" aria-hidden="true" tabindex="-1"></a>            count <span class="op">+=</span><span class="dv">1</span></span>
<span id="cb9-15"><a href="#cb9-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-16"><a href="#cb9-16" aria-hidden="true" tabindex="-1"></a>            <span class="co"># skip examples already processed on previous runs</span></span>
<span id="cb9-17"><a href="#cb9-17" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> count <span class="op">&lt;</span> success_count:</span>
<span id="cb9-18"><a href="#cb9-18" aria-hidden="true" tabindex="-1"></a>                <span class="cf">continue</span></span>
<span id="cb9-19"><a href="#cb9-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-20"><a href="#cb9-20" aria-hidden="true" tabindex="-1"></a>            <span class="bu">print</span>(<span class="ss">f'Currently processing line </span><span class="sc">{</span>count<span class="sc">}</span><span class="ss"> of </span><span class="sc">{</span>total_count<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb9-21"><a href="#cb9-21" aria-hidden="true" tabindex="-1"></a>            clear_output(wait<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb9-22"><a href="#cb9-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-23"><a href="#cb9-23" aria-hidden="true" tabindex="-1"></a>            <span class="co"># get the next example from the file</span></span>
<span id="cb9-24"><a href="#cb9-24" aria-hidden="true" tabindex="-1"></a>            data <span class="op">=</span> json.loads(line)</span>
<span id="cb9-25"><a href="#cb9-25" aria-hidden="true" tabindex="-1"></a>            prompt <span class="op">=</span> data[<span class="st">"prompt"</span>]</span>
<span id="cb9-26"><a href="#cb9-26" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-27"><a href="#cb9-27" aria-hidden="true" tabindex="-1"></a>            <span class="co"># add the example to a copy of the instruction set</span></span>
<span id="cb9-28"><a href="#cb9-28" aria-hidden="true" tabindex="-1"></a>            msgs <span class="op">=</span> instructions.copy()</span>
<span id="cb9-29"><a href="#cb9-29" aria-hidden="true" tabindex="-1"></a>            msgs.append({<span class="st">"role"</span>: <span class="st">"user"</span>, <span class="st">"content"</span>: <span class="ss">f'start --&gt;</span><span class="sc">{</span>prompt<span class="sc">}</span><span class="ss">'</span>})</span>
<span id="cb9-30"><a href="#cb9-30" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-31"><a href="#cb9-31" aria-hidden="true" tabindex="-1"></a>            <span class="co"># send the request</span></span>
<span id="cb9-32"><a href="#cb9-32" aria-hidden="true" tabindex="-1"></a>            res <span class="op">=</span> chat_with_backoff(model <span class="op">=</span> <span class="st">"gpt-3.5-turbo"</span>, messages <span class="op">=</span> msgs)</span>
<span id="cb9-33"><a href="#cb9-33" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-34"><a href="#cb9-34" aria-hidden="true" tabindex="-1"></a>            <span class="co"># if the request failed for some reason, skip the example</span></span>
<span id="cb9-35"><a href="#cb9-35" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> <span class="kw">not</span> res:</span>
<span id="cb9-36"><a href="#cb9-36" aria-hidden="true" tabindex="-1"></a>                <span class="cf">continue</span></span>
<span id="cb9-37"><a href="#cb9-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-38"><a href="#cb9-38" aria-hidden="true" tabindex="-1"></a>            <span class="co"># get the response and write the example back to disk</span></span>
<span id="cb9-39"><a href="#cb9-39" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> res[<span class="st">"choices"</span>][<span class="dv">0</span>][<span class="st">"message"</span>][<span class="st">"content"</span>] <span class="op">==</span> <span class="st">"no"</span>:</span>
<span id="cb9-40"><a href="#cb9-40" aria-hidden="true" tabindex="-1"></a>                <span class="co"># for negative messages, re-write the completion as ` nil`</span></span>
<span id="cb9-41"><a href="#cb9-41" aria-hidden="true" tabindex="-1"></a>                data[<span class="st">"completion"</span>] <span class="op">=</span> <span class="st">" nil"</span></span>
<span id="cb9-42"><a href="#cb9-42" aria-hidden="true" tabindex="-1"></a>            out_file.write(json.dumps(data) <span class="op">+</span> <span class="st">'</span><span class="ch">\n</span><span class="st">'</span>) </span>
<span id="cb9-43"><a href="#cb9-43" aria-hidden="true" tabindex="-1"></a>            out_file.flush()</span>
<span id="cb9-44"><a href="#cb9-44" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb9-45"><a href="#cb9-45" aria-hidden="true" tabindex="-1"></a>            <span class="co"># save progress for restart</span></span>
<span id="cb9-46"><a href="#cb9-46" aria-hidden="true" tabindex="-1"></a>            success_count <span class="op">=</span> count</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="example-validation" class="level3">
<h3 class="anchored" data-anchor-id="example-validation">Example validation</h3>
<p>Finally, we will use a CLI tool provided by OpenAI to validate our training examples and split them into two files. The tool does the following for us:</p>
<ul>
<li>Ensures that all prompts end with the same suffix.</li>
<li>Removes examples that use too many tokens.</li>
<li>Deletes duplicated examples.</li>
</ul>
<p>We can run the CLI tool directly from a Python Jupyter Notebook with the code below.</p>
<div class="cell">
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> types <span class="im">import</span> SimpleNamespace</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>args <span class="op">=</span> SimpleNamespace(<span class="bu">file</span><span class="op">=</span><span class="st">'examples_refined.jsonl'</span>, quiet<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>openai.cli.FineTune.prepare_data(args)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The output of the above command should be two files:</p>
<ul>
<li><code>examples_refined_prepared_train.jsonl</code> -&gt; We will use this to fine-tune our model</li>
<li><code>examples_refined_prepared_valid.jsonl</code> -&gt; We will use this to validate our fine-tuned model</li>
</ul>
<div class="callout callout-style-default callout-important callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Important
</div>
</div>
<div class="callout-body-container callout-body">
<p>The output from the CLI tool will include a command for starting a model fine-tuning. I recommend you skip those instructions and use mine below instead.</p>
</div>
</div>
</section>
</section>
<section id="model-fine-tuning-1" class="level1">
<h1>Model Fine-Tuning</h1>
<p>Now that we have refined training examples, we can fine-tune a model.</p>
<section id="upload-training-data" class="level3">
<h3 class="anchored" data-anchor-id="upload-training-data">Upload training data</h3>
<p>First, we upload the refined examples to OpenAI. We must ensure the file has been successfully uploaded before moving on to the next step.</p>
<div class="cell">
<details open="">
<summary>Code: upload training data</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>training_file_name <span class="op">=</span> <span class="st">"examples_refined_prepared_train.jsonl"</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="co"># start the file upload</span></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a>training_file_id <span class="op">=</span> openai.cli.FineTune._get_or_upload(training_file_name, <span class="va">True</span>)</span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a><span class="co"># Poll and display the upload status until it finishes</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    time.sleep(<span class="dv">2</span>)</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    file_status <span class="op">=</span> openai.File.retrieve(training_file_id)[<span class="st">"status"</span>]</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Upload status: </span><span class="sc">{</span>file_status<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> file_status <span class="kw">in</span> [<span class="st">"succeeded"</span>, <span class="st">"failed"</span>, <span class="st">"processed"</span>]:</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="create-a-fine-tuning-job" class="level3">
<h3 class="anchored" data-anchor-id="create-a-fine-tuning-job">Create a fine-tuning job</h3>
<p>Next, we create a fine-tuning job using the file_id from the upload.</p>
<p>When doing fine-tuning, you need to choose a base model to start from. The current options are:</p>
<ul>
<li><code>ada</code> -&gt; Capable of very simple tasks, usually the fastest model in the GPT-3 series, and lowest cost.</li>
<li><code>babbage</code> -&gt; Capable of straightforward tasks. Very fast and low cost.</li>
<li><code>curie</code> -&gt; Very capable, but faster and lower cost than Davinci.</li>
<li><code>davinci</code> -&gt; Most capable GPT3 model. It is more expensive to train and run in production.</li>
</ul>
<p>You must also choose the number of epochs to train the model for. An epoch refers to one full cycle through the training dataset.</p>
<p>The cost of fine-tuning is based on the base model, the number of examples, and the number of epochs you will run. Generally, doubling the number of epochs doubles the training cost. However, there is a trade-off to consider here because cheaper base models will be cheaper to run in production.</p>
<p>With the example set I was using for BeepGPT, I found that eight epochs on <code>curie</code> produced a model with a similar capability as four on <code>davinci</code>. Depending on your use case, you may or may not have a similar result.</p>
<div class="cell" data-execution_count="17">
<details open="">
<summary>Code: create a fine-tuning job</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>create_args <span class="op">=</span> {</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"training_file"</span>: training_file_id,</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"model"</span>: <span class="st">"davinci"</span>,</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"n_epochs"</span>: <span class="dv">4</span>,</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"suffix"</span>: <span class="st">"beep-gpt"</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>}</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Create the fine-tune job and retrieve the job ID</span></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>resp <span class="op">=</span> openai.FineTune.create(<span class="op">**</span>create_args)</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>job_id <span class="op">=</span> resp[<span class="st">"id"</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="wait-for-the-job-to-finish" class="level3">
<h3 class="anchored" data-anchor-id="wait-for-the-job-to-finish">Wait for the job to finish</h3>
<p>After the fine-tuning job has been created, we need to wait for it to start processing and then for it to finish.</p>
<p>Depending on the current backlog at OpenAI, I’ve seen that jobs can take up to a dozen hours to start.</p>
<p>After the job starts successfully, you can see its status and wait for it to finish. This can also take a long time. When using <code>davinci</code> with four epochs, I estimate about 1 hour per 1000 training examples.</p>
<div class="cell">
<details>
<summary>Code: wait for the job to finish</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Poll and display the fine-tuning status until it finishes</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> IPython.display <span class="im">import</span> clear_output</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    time.sleep(<span class="dv">5</span>)</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    job_details <span class="op">=</span> openai.FineTune.retrieve(<span class="bu">id</span><span class="op">=</span>job_id)</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Job status: </span><span class="sc">{</span>job_details[<span class="st">"status"</span>]<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f'Job events: </span><span class="sc">{</span>job_details[<span class="st">"events"</span>]<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a>    clear_output(wait<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> job_details[<span class="st">"status"</span>] <span class="op">==</span> <span class="st">"succeeded"</span>:</span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>        model_id <span class="op">=</span> job_details[<span class="st">"fine_tuned_model"</span>]</span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'Successfully fine-tuned model with ID: </span><span class="sc">{</span>model_id<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-16"><a href="#cb13-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> job_details[<span class="st">"status"</span>] <span class="kw">in</span> [<span class="st">"failed"</span>, <span class="st">"succeeded"</span>]:</span>
<span id="cb13-17"><a href="#cb13-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">break</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="using-the-fine-tuned-model" class="level3">
<h3 class="anchored" data-anchor-id="using-the-fine-tuned-model">Using the fine-tuned model</h3>
<p>Now that we have a finished model, we can try sending a few prompts and see if it recommends alerting any users. We can use the validation file for this.</p>
<p>See the <a href="https://platform.openai.com/docs/api-reference/completions/create">OpenAI docs</a> for info on the parameters we send to the Completion API.</p>
<div class="cell">
<details>
<summary>Code: send requests to the model</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># choose which row in the validation file to send</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>row <span class="op">=</span> <span class="dv">6</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>count <span class="op">=</span> <span class="dv">0</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="ss">f'examples_refined_prepared_valid.jsonl'</span>, <span class="st">'r'</span>) <span class="im">as</span> in_file:</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> line <span class="kw">in</span> in_file:</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>        count <span class="op">+=</span><span class="dv">1</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> count <span class="op">&lt;</span> row:</span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">continue</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>        data <span class="op">=</span> json.loads(line)</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>        prompt <span class="op">=</span> data[<span class="st">"prompt"</span>]</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>        completion <span class="op">=</span> data[<span class="st">"completion"</span>]</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># this is the text we send to the model for it to </span></span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># determine if we should alert a user</span></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'Prompt: </span><span class="sc">{</span>prompt<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>        <span class="co"># this is the user (or nil) we would have expected </span></span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># for the response (from the validation file)</span></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'Completion: </span><span class="sc">{</span>completion<span class="sc">}</span><span class="ss">'</span>)</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>        <span class="co"># this is the response from the model. The `text` field contains </span></span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>        <span class="co"># the actual prediction. The `logprobs` array contains the </span></span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># log-probability from the 5 highest potential matches.</span></span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f'Prediction:'</span>)</span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>        openai.Completion.create(model<span class="op">=</span>model_id, prompt<span class="op">=</span>prompt, max_tokens<span class="op">=</span><span class="dv">1</span>, logprobs<span class="op">=</span><span class="dv">5</span>, temperature<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
</section>
<section id="model-validation" class="level1">
<h1>Model Validation</h1>
<p>You can run your model over the full validation data set to validate it. Then, use a basic data science performance measurement to check the quality of your model. I calculate the model’s <a href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score">F1 Score</a> in the example below, but you can use other performance indicators if desired.</p>
<div class="cell">
<details>
<summary>Code: run the model on the validation data set</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> <span class="bu">open</span>(<span class="ss">f'examples_refined_prepared_valid.jsonl'</span>, <span class="st">'r'</span>) <span class="im">as</span> in_file:</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> <span class="bu">open</span>(<span class="ss">f'examples_refined_prepared_valid_pred.jsonl'</span>, <span class="st">'w'</span>) <span class="im">as</span> out_file:</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> line <span class="kw">in</span> in_file:</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>            <span class="co"># for each example in the validation input file</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a>            <span class="co"># run the completion API using our fine-tuned model</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a>            <span class="co"># write the results to the output file</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>            pred <span class="op">=</span> openai.Completion.create(model<span class="op">=</span>model_id, prompt<span class="op">=</span>prompt, max_tokens<span class="op">=</span><span class="dv">1</span>, logprobs<span class="op">=</span><span class="dv">5</span>, temperature<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>            data[<span class="st">"prediction"</span>] <span class="op">=</span> pred</span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>            out_file.write(json.dumps(data) <span class="op">+</span> <span class="st">'</span><span class="ch">\n</span><span class="st">'</span>) </span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<details>
<summary>Code: calculate the f1 score</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load the results from above into a dataframe</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> pandas.read_json(<span class="ss">f'examples_refined_prepared_valid_pred.jsonl'</span>, lines<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>df[<span class="st">"test"</span>] <span class="op">=</span> <span class="va">None</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>df[<span class="st">"pred"</span>] <span class="op">=</span> <span class="va">None</span></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a><span class="co"># fill the test and pred columns in the dataframe </span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a><span class="co"># from the Completion API results</span></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(df)):</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>    completions <span class="op">=</span> df[<span class="st">'completion'</span>][i].strip().split()</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>    df.at[i, <span class="st">"test"</span>] <span class="op">=</span> completions</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a>    prediction <span class="op">=</span> df[<span class="st">'prediction'</span>][i]</span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="st">"choices"</span> <span class="kw">in</span> prediction:</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>        predictions <span class="op">=</span> prediction[<span class="st">"choices"</span>][<span class="dv">0</span>][<span class="st">"text"</span>].strip().split()</span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a>        df.at[i, <span class="st">"pred"</span>] <span class="op">=</span> predictions</span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a><span class="co"># drop rows where "pred" is null</span></span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>df <span class="op">=</span> df[df.pred.notnull()]</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a><span class="co"># compute the 'macro' F1 score. also can try </span></span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a><span class="co"># the 'micro' or 'weighted' score based on need</span></span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> f1_score</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>f1 <span class="op">=</span> f1_score(df[<span class="st">'test'</span>], df[<span class="st">'pred'</span>], average<span class="op">=</span><span class="st">'macro'</span>)  </span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>f1</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>An F1 score is a measure of a model’s accuracy, and it takes into account both precision and recall.</p>
<ul>
<li>Precision is the number of true positive predictions divided by the total number of positive predictions. It measures how accurate the model’s positive predictions are.</li>
<li>Recall is the number of true positive predictions divided by the total number of positive cases. It measures how well the model identifies positive cases.</li>
</ul>
<p>The F1 score is the harmonic mean of precision and recall. F1 scores range from 0 to 1, with a score of 1 indicating perfect precision and recall and 0 indicating poor performance. As a general rule of thumb, an F1 score of 0.7 or higher is often considered good. <span class="citation" data-cites="f1score">(<a href="#ref-f1score" role="doc-biblioref">Neri Van Otten 2023</a>)</span></p>
</section>
<section id="conclusion" class="level1">
<h1>Conclusion</h1>
<p>As we wrap up this post, it’s evident that fine-tuning large language models can be a promising endeavor, even for those of us without a Data Science background. Through the example of <a href="https://github.com/kaskada-ai/beep-gpt">BeepGPT</a>, we saw that the process, while requiring patience and iteration, can produce models that offer valuable insights.</p>
<section id="key-takeaways" class="level4">
<h4 class="anchored" data-anchor-id="key-takeaways">Key Takeaways</h4>
<ol type="1">
<li><p><strong>Experimentation is vital</strong>: Training a model that works efficiently requires much trial and error. As showcased with BeepGPT, sometimes the fifth attempt may be the charm!</p></li>
<li><p><strong>Experimentation is vital</strong>: Training a model that works efficiently requires much trial and error. As showcased with BeepGPT, sometimes the fifth attempt may be the charm!</p></li>
<li><p><strong>Few-Shot Learning – A Game-Changer</strong>: In the midst of refining BeepGPT, I stumbled upon an innovative trick: leveraging few-shot learning to elevate the quality of our training data. While it seemed unconventional at first, the results were staggering. This technique might just revolutionize the way we fine-tune LLMs in the future.</p></li>
<li><p><strong>Prioritize Data Quality</strong>: Even with tricks up our sleeve, the core principle remains - garbage in equals garbage out. The essence of a model’s efficiency lies in the caliber of data it’s trained on.</p></li>
<li><p><strong>Comprehensive Training Examples</strong>: Building and refining a large set of training examples ensures a model can predict accurately in real-world scenarios.</p></li>
</ol>
</section>
<section id="next-steps" class="level4">
<h4 class="anchored" data-anchor-id="next-steps">Next Steps</h4>
<ol type="1">
<li><p><strong>Deep Dive into Tools</strong>: In future posts, I’ll explore <a href="http://kaskada.io/kaskada">Kaskada</a> more deeply, showcasing how it aids in simplifying the process of gathering and refining training examples.</p></li>
<li><p><strong>Optimization</strong>: As technology evolves, so do the tools and strategies for model fine-tuning. I’ll explore strategies to optimize the training process, from reducing costs to increasing accuracy.</p></li>
<li><p><strong>Model Deployment</strong>: With a validated model, the following steps involve deploying it into real-world applications. In upcoming posts, I’ll look at strategies and best practices for integrating models with various platforms.</p></li>
<li><p><strong>Feedback Loop</strong>: Continuous improvement is a hallmark of successful machine learning. We’ll explore ways to gather feedback on model predictions, further refining and improving it over time.</p></li>
</ol>
<p>In conclusion, the landscape of data science and machine learning is more accessible than ever. With the right tools, patience, and curiosity, even software engineers with minimal data science experience can harness the power of advanced models to provide tangible value. Whether you’re a seasoned pro or a newcomer like me, the journey of discovery and innovation in this space is just beginning.</p>
</section>
</section>
<section id="references" class="level1">
<h1>References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-f1score" class="csl-entry" role="listitem">
Neri Van Otten. 2023. <span>“<a href="https://spotintelligence.com/2023/05/08/f1-score/">F1 Score The Ultimate Guide: Formulas, Explanations, Examples, Advantages, Disadvantages, Alternatives &amp; Python Code</a>.”</span>
</div>
<div id="ref-gigo" class="csl-entry" role="listitem">
Ron Ozminkowski, PhD. 2021. <span>“<a href="https://towardsdatascience.com/garbage-in-garbage-out-721b5b299bc1">Garbage In, Garbage Out: Saving the World Is Just One Good Reason to Address This Common Problem</a>.”</span>
</div>
<div id="ref-llm" class="csl-entry" role="listitem">
Sean Michael Kerner. 2023. <span>“<a href="https://www.techtarget.com/whatis/definition/large-language-model-LLM">Definition: Large Language Model</a>.”</span>
</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var filterRegex = new RegExp("^(?:http:|https:)\/\/epinzur\.github\.io\/");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
          // default icon
          link.classList.add("external");
      }
    }
});
</script>
<script src="https://utteranc.es/client.js" repo="epinzur/epinzur.github.io" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center"><div class="cookie-consent-footer"><a href="#" id="open_preferences_center">Cookie Preferences</a></div></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>



</body></html>